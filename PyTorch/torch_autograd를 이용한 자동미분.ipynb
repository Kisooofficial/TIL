{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNFc4f7rgieHCZHDwZ6we3T"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"e-J6XMLMqM00","executionInfo":{"status":"ok","timestamp":1695113983222,"user_tz":-540,"elapsed":9105,"user":{"displayName":"김기수","userId":"05232178571319528981"}}},"outputs":[],"source":["## torch.autograd라는 자동 미분 엔진이 내장되어 있음. 모든 계산 그래프에 대한 변화도의 자동 계산을 지원함\n","import torch\n","\n","x = torch.ones(5)\n","y = torch.zeros(3)\n","w = torch.randn(5, 3, requires_grad = True) ## 학습해야하는 파라미터들에 대해서 손실 함수의 변화도 계산 위한 절차\n","b = torch.randn(3, requires_grad = True) ## 마찬가지\n","z = torch.matmul(x, w) + b\n","loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)"]},{"cell_type":"code","source":["## grad_fn은 순전파랑 역전파 계산하는 방법을 알고 있음\n","print(f'Gradient function for z = {z.grad_fn}')\n","print(f'Gradient function for loss = {loss.grad_fn}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xo83IIGLqm6-","executionInfo":{"status":"ok","timestamp":1695114134711,"user_tz":-540,"elapsed":10,"user":{"displayName":"김기수","userId":"05232178571319528981"}},"outputId":"fcc71fd9-6b87-4084-f083-57220b74a560"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Gradient function for z = <AddBackward0 object at 0x7fd8fb7ae7d0>\n","Gradient function for loss = <BinaryCrossEntropyWithLogitsBackward0 object at 0x7fd8fb7afd30>\n"]}]},{"cell_type":"code","source":["## 가중치 최적화 위해 매개변수에 대한 손실함수의 도함수를 계산해야함.\n","## 각 매개변수 별로 loss에 대해서 w와 b의 편미분이 필요함\n","loss.backward()\n","print(w.grad)\n","print(b.grad)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KWCEMGikrQAd","executionInfo":{"status":"ok","timestamp":1695114182178,"user_tz":-540,"elapsed":4,"user":{"displayName":"김기수","userId":"05232178571319528981"}},"outputId":"868854a8-d571-49db-d8d9-ebca250ab5c7"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0.2802, 0.1434, 0.2431],\n","        [0.2802, 0.1434, 0.2431],\n","        [0.2802, 0.1434, 0.2431],\n","        [0.2802, 0.1434, 0.2431],\n","        [0.2802, 0.1434, 0.2431]])\n","tensor([0.2802, 0.1434, 0.2431])\n"]}]},{"cell_type":"code","source":["## requires_grad = True의 경우에는 연산 기록을 추적하고 변화도 계산을 지원함.\n","## 만약, 순전파 연산만 필요하다면, 이러한 추적이나 지원이 필요하지 않을 수도 있음. => torch.no_grad()\n","\n","z = torch.matmul(x, w) + b\n","print(z.requires_grad)\n","\n","with torch.no_grad():\n","    z = torch.matmul(x, w) + b\n","print(z.requires_grad)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sHZNLRW1rbnT","executionInfo":{"status":"ok","timestamp":1695114303197,"user_tz":-540,"elapsed":9,"user":{"displayName":"김기수","userId":"05232178571319528981"}},"outputId":"99b26c2c-6853-45c6-cc45-0bffc9b30a60"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["True\n","False\n"]}]},{"cell_type":"code","source":["z = torch.matmul(x, w) + b\n","z_det = z.detach() ## 역전파 안함\n","print(z.requires_grad)\n","print(z_det.requires_grad)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"unxRDgN0r5HD","executionInfo":{"status":"ok","timestamp":1695114333451,"user_tz":-540,"elapsed":6,"user":{"displayName":"김기수","userId":"05232178571319528981"}},"outputId":"38682a8b-cce7-4c90-a8e4-4a007ac8a78e"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["True\n","False\n"]}]},{"cell_type":"code","source":["#### 변화도 추적을 멈춰야 하는 경우 ####\n","## 신경망의 일부 매개변수를 고정된 매개변수로 표시하는 경우\n","## 변화도를 추적하지 않는 텐서의 연산이 더 효율적이기 때문에, 순전파 단계만 수행할 때 연산 속도가 향상됌."],"metadata":{"id":"PD-D1Z9UsE0h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## autograd는 데이터(텐서) 및 실행된 모든 연산들의 기록을 Function 객체로 구성된 방향성 비순환 그래프에 저장함\n","# 순전파 -> 요청된 연산을 수행해서 결과 텐서를 계산하고 DAG에 연산의 변화도 기능을 유지함\n","# 역전파 -> DAG의 뿌리(결과)에서 .backward()가 호추로딜 때 시작함\n","## .grad_fn으로부터 변화도 계산, 각 텐서의 .grad 속성에 계산 결과를 쌓고(accumulate), 연쇄법칙을 이용해서 모든 잎 텐서까지 전파\n","\n","## backward에 매개변수가 들어가는 이유 -> 결과 자체가 행렬이기 때문에\n","inp = torch.eye(4, 5, requires_grad = True)\n","out = (inp + 1).pow(2).t()\n","out.backward(torch.ones_like(out), retain_graph = True) ## 여기서 여러번의 backward 계산이 필요함\n","print(f'First call\\n{inp.grad}')\n","out.backward(torch.ones_like(out), retain_graph = True)\n","print(f'\\nSecond call\\n{inp.grad}')\n","inp.grad.zero_()\n","out.backward(torch.ones_like(out), retain_graph = True)\n","print(f'\\nCall after zeroing gradients\\n{inp.grad}')\n","\n","## 역방향 전파 수행 시, PyTorch가 변화도를 누적해주기 때문임. 계산된 변화도의 값이 연산 그래프의 모든 노드의 grad 속성에 추가\n","## 따라서, 제대로 된 변화도를 계산하려면 grad 속성을 0으로 만들어 줘야 함."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2gIjZ6fBsAhh","executionInfo":{"status":"ok","timestamp":1695114803322,"user_tz":-540,"elapsed":7,"user":{"displayName":"김기수","userId":"05232178571319528981"}},"outputId":"5b2e7d8a-2605-41bf-ca08-935c760ca267"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["First call\n","tensor([[4., 2., 2., 2., 2.],\n","        [2., 4., 2., 2., 2.],\n","        [2., 2., 4., 2., 2.],\n","        [2., 2., 2., 4., 2.]])\n","\n","Second call\n","tensor([[8., 4., 4., 4., 4.],\n","        [4., 8., 4., 4., 4.],\n","        [4., 4., 8., 4., 4.],\n","        [4., 4., 4., 8., 4.]])\n","\n","Call after zeroing gradients\n","tensor([[4., 2., 2., 2., 2.],\n","        [2., 4., 2., 2., 2.],\n","        [2., 2., 4., 2., 2.],\n","        [2., 2., 2., 4., 2.]])\n"]}]}]}